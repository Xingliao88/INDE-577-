{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0761d0f9",
   "metadata": {},
   "source": [
    "# Neural Network\n",
    "## Working Principle Introduction\n",
    "Neural networks consist of layers of interconnected nodes or 'neurons', each layer designed to perform specific transformations on its inputs.Signals travel from the first layer (the input layer), through one or more 'hidden layers' to the last layer (the output layer), where a prediction is made.Each neuron in one layer is connected to neurons in the next layer via weights. Learning involves adjusting these weights based on the error of the output compared to the expected result.\n",
    "\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"Neuralnet.jpg\" alt=\"alt text\" width=\"450\"/>\n",
    "</div>\n",
    "\n",
    "The image depicts a neural network with an input layer, two hidden layers, and an output layer, showing how data flows from the inputs through interconnected neurons to produce an output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d25b6f",
   "metadata": {},
   "source": [
    "## Data Processing\n",
    "In the data preprocessing for my neural network model, I loaded the penguin dataset, cleaned it, applied label encoding to categorical features, and standardized all features. Then, I split the data into training and test sets and one-hot encoded the target labels to prepare for neural network training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e416b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load and preprocess the penguin dataset\n",
    "penguin_data = sns.load_dataset(\"penguins\")\n",
    "penguin_data.dropna(inplace=True)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "for col in ['species', 'island', 'sex']:\n",
    "    penguin_data[col] = label_encoder.fit_transform(penguin_data[col])\n",
    "\n",
    "X = penguin_data.drop('species', axis=1).values\n",
    "y = penguin_data['species'].values\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "train_X, test_X, train_y, test_y = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Flatten the data and one-hot encode the labels\n",
    "num_classes = len(np.unique(train_y))\n",
    "flat_train_X = [x.reshape(-1, 1) for x in train_X]\n",
    "flat_test_X = [x.reshape(-1, 1) for x in test_X]\n",
    "\n",
    "onehot_train_y = [np.eye(num_classes)[int(yi)].reshape(-1, 1) for yi in train_y]\n",
    "onehot_test_y = [np.eye(num_classes)[int(yi)].reshape(-1, 1) for yi in test_y]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d1ebf8",
   "metadata": {},
   "source": [
    "## Neural Network Implementation and Functions\n",
    "In this section, I've defined the core functions and classes required to construct a neural network from scratch. This includes the sigmoid activation function and its derivative, a function to compute mean squared error (MSE), and an initialization function to set up the weights and biases with appropriate dimensions. I then crafted a DenseNetwork class encapsulating the network's forward pass, which calculates the activations, and the backward pass, which performs backpropagation by computing gradients and updating the weights. Training and prediction methods are also provided to fit the model to the training data and to predict on new data, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "274d7c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network Functions\n",
    "def sigmoid(z):\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def d_sigmoid(z):\n",
    "    return sigmoid(z)*(1.0 - sigmoid(z))\n",
    "\n",
    "def mse(a, y):\n",
    "    return 0.5 * np.sum((a - y)**2).item()\n",
    "\n",
    "def initialize_weights(layers):\n",
    "    W = [[0.0]]\n",
    "    B = [[0.0]]\n",
    "    for i in range(1, len(layers)):\n",
    "        w_temp = np.random.randn(layers[i], layers[i-1]) * np.sqrt(2 / layers[i-1])\n",
    "        b_temp = np.random.randn(layers[i], 1) * np.sqrt(2 / layers[i-1])\n",
    "        W.append(w_temp)\n",
    "        B.append(b_temp)\n",
    "    return W, B\n",
    "\n",
    "# Dense Network Class\n",
    "class DenseNetwork(object):\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "        self.W, self.B = initialize_weights(layers)\n",
    "\n",
    "    def forward_pass(self, xi):\n",
    "        Z = [[0.0]]\n",
    "        A = [xi]\n",
    "        for i in range(1, len(self.W)):\n",
    "            z = self.W[i] @ A[i-1] + self.B[i]\n",
    "            Z.append(z)\n",
    "            a = sigmoid(z)\n",
    "            A.append(a)\n",
    "        return Z, A\n",
    "\n",
    "    def backward_pass(self, xi, yi, Z, A, alpha):\n",
    "        L = len(self.layers) - 1\n",
    "        deltas = dict()\n",
    "\n",
    "        # Output error\n",
    "        output_error = (A[L] - yi) * d_sigmoid(Z[L])\n",
    "        deltas[L] = output_error\n",
    "\n",
    "        # Hidden layer errors\n",
    "        for i in range(L-1, 0, -1):\n",
    "            deltas[i] = (self.W[i+1].T @ deltas[i+1]) * d_sigmoid(Z[i])\n",
    "\n",
    "        # Update weights and biases\n",
    "        for i in range(1, L+1):\n",
    "            self.W[i] -= alpha * deltas[i] @ A[i-1].T\n",
    "            self.B[i] -= alpha * deltas[i]\n",
    "\n",
    "    def train(self, X_train, y_train, alpha=0.046, epochs=4):\n",
    "        for k in range(epochs):\n",
    "            for xi, yi in zip(X_train, y_train):\n",
    "                Z, A = self.forward_pass(xi)\n",
    "                self.backward_pass(xi, yi, Z, A, alpha)\n",
    "\n",
    "    def predict(self, xi):\n",
    "        _, A = self.forward_pass(xi)\n",
    "        return np.argmax(A[-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcf1ae9",
   "metadata": {},
   "source": [
    "## Network Training and Performance Evaluation\n",
    "In this section, I initialized the neural network with a specified layer structure and trained it using the flattened training data and one-hot encoded labels. I set the learning rate and the number of epochs for the training process. After training, I predicted the outcomes on the test set and computed the accuracy of the model by comparing the predicted results against the actual labels. The calculated accuracy rate demonstrates the effectiveness of the neural network on the given task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a2d1c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.82\n"
     ]
    }
   ],
   "source": [
    "# Network Initialization and Training\n",
    "input_size = train_X.shape[1]\n",
    "network_layers = [input_size, 30, 30, num_classes]\n",
    "net = DenseNetwork(layers=network_layers)\n",
    "net.train(flat_train_X, onehot_train_y, alpha=0.046, epochs=4)\n",
    "\n",
    "# Prediction and Accuracy Calculation\n",
    "y_pred = [net.predict(xi) for xi in flat_test_X]\n",
    "actual_y = [np.argmax(yi) for yi in onehot_test_y]\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = sum(pred == actual for pred, actual in zip(y_pred, actual_y)) / len(actual_y)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f3821e",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Following the successful training of the neural network with a defined architecture and hyperparameters, I evaluated its performance on the test data. The model achieved a noteworthy accuracy, reflecting its capability to generalize and make predictions on unseen data. This underscores the potential of the implemented neural network in classifying complex patterns within the dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
